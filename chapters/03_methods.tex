% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Methods}\label{chapter:methods}

\section{Dataset}
Datasets such as CAMS (cite) or TNO (cite) provide GHG emissions in a 0.1 times 0.05 lat times long resolution which corresponds to ~5 by ~5 kilometers.
This resolution is very low and urban data cannot effectively be extracted.
For instance, a city contained within a 30km by 30km grid, would contain 6 by 6 by 15 values.
This is not enough to draw any meaningful information and conclusion. 
Therefore, this thesis focues on the high res $2015$ and $2018$ TNO datasets are used.
These are two high resolution, roughly 1km by 1km, emission inventory and thus high resolution emission fields in urban environments can be extracted.
The inventory contains GHG emissions for each GNFR sector (cite), for each coordinate in a defined grid within Europe.
For this thesis, only CO2 from fossil fuels are considered.
However, the presented approaches can be directly applied for all types of GHG sources.

\subsection{Urban Emission Fields}
Urban environments differ from non-urban environments in terms of their emissions.
For instance, urban environments often have a city center that generates a large portion of emissions as seen for example in Munich (picture of Munich).
Therefore, as the focus of this thesis are urban environments, they have to be filtered from the TNO datasets.
For this, a number of cities are selected from a public database (cite here).
Cities are filtered according to their population size.
For this thesis, the population threshold is set to $100000$.
The number of extracted cities is $2xx$.
A list of all cities with the corresponding coordinates is appended in the appendix.

From the filtered cities, the coordinates are used to extract emission fields.
From the TNO dataset, $n$ by $n$ grids around the city center are extracted.
This corresponds to a dimension of n km by n km.
While most cities are not even close to this size, this allows to later crop the fields to a desired size.
For this thesis, emission fields have $32$ by $32$.

The number of extracted emission fields is not enough to train a generative model.
Thus, temporal scaling factors from (cite) are used to generate more samples.
Scaling factors are applied to individual sectors.
This results in $24 * 7 * 12 = 2016$ samples per city per year.
The resulting dataset size is thus $2xx * 2 * 2016 = $ samples.
To reduce the memory overhead, scaling factors are applied at sampling time and only the original data is kept in memory.

\subsection{Dataset split}
The emission fields are split into training, validation, and test data.
The training data is used to train the model and updates its weights and makes up $\sim  70\%$ of the data.
The validation data is used to evaluate the generalization capabilities of the model during training and makes up $\sim  15\%$.
The test data is used for subsequent experiments and evaluations and makes up $\sim  15\%$.

For splitting, the emission fields are first sorted alphabetically by the city name.
Then, the first $15\%$ of cities are assigned to the test data.
This makes the split deterministic and thus reproducible.

The remaining cities are then randomly shuffled.
Finally, the first $\frac{0.15}{0.85}$ of cities are assigned to the validation set.
The remaining cities are assigned to the training set.

Maybe make a diagram here.

The number of samples in training, validation, and test set is given in table (make table).
A list of the cities in the test set is appended in appendix (add appendix).

\subsection{Data Augmentation}
To improve the generalization abilities of the model CITE, common image augmentation techniques are applied (to cite) to the emission fields in the training data.
First, random cropping is used.
Furthermore, a random horizontal flip and vertical flip are applied with probability $0.5$ each.
Lastly, emission fields are randomly rotated by 90$\circ$ with probability of $0.5$.
This results in $8$ different transformations, with one of them being the original emission field.

The validation and test emission fields are not transformed, except for a center crop.

\subsection{Scaling}
Scaling is an important factor for machine learning.
Large scale data can make training converge slow and instable (cite).
Furthermore, regularization techniques such as batch normalization rely on scaled values (cite).
A common technique for scaling values is min max scaling (cite).
However, min max scaling is sensitive to outliers and thus not ideal for emission inventories where range of emissions within a city can largely differ.
Instead, robust scaling (cite) can be applied.
To determine a good scaling factor, the $95$th percentile of values is determined for each city in the training set.
The inverse of the average is then used as the scaling factor.
The resulting scaling factor is thus $\frac{1 a}{2.5 * 10^6 kg}$

\section{Variational Autoencoder}

\subsection{Architecture}

\section{Evaluation}


