% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Methods}\label{chapter:methods}

\section{Dataset}
Datasets such as CAMS (cite) or TNO (cite) provide GHG emissions in a 0.1 times 0.05 lat times long resolution which corresponds to ~5 by ~5 kilometers.
This resolution is very low and urban data cannot effectively be extracted.
For instance, a city contained within a 30km by 30km grid, would contain 6 by 6 cells.
This is not enough to draw any meaningful information and conclusion. 
Therefore, this thesis focues on the high res $2015$ and $2018$ TNO datasets are used.
These are two high resolution, roughly 1km by 1km, emission inventory and thus high resolution emission fields in urban environments can be extracted.
The inventory contains GHG emissions for each GNFR sector (cite), with the road sector $F$ being split into $4$ sub sectors, for each coordinate in a defined grid within Europe.
This results in $15$ emission values per gas per cell.
For this thesis, only CO2 from fossil fuels are considered.
However, the presented approaches can be directly applied for all types of GHG sources.

\subsection{Urban Emission Fields}
Urban environments differ from non-urban environments in terms of their emissions.
For instance, urban environments often have a city center that generates a large portion of emissions as seen for example in Munich (picture of Munich).
Therefore, as the focus of this thesis are urban environments, they have to be filtered from the TNO datasets.
For this, a number of cities are selected from a public database by OpenDatasoft (cite here).
Cities are filtered according to their population size.
For this thesis, the population threshold is set to $100000$.
The resulting number of extracted cities is $2xx$.
A list of all cities with the corresponding coordinates is appended in the appendix.

From the filtered cities, the coordinates are used to extract emission fields.
From the TNO dataset, $n$ by $n$ grids around the city center are extracted.
This corresponds to a dimension of n km by n km.
While most cities are not even close to this size, this allows to later crop the fields to a desired size.
For this thesis, emission fields have $32$ by $32$.

The number of extracted emission fields is not enough to train a generative model.
Thus, temporal scaling factors from (cite) are used to generate more samples.
Scaling factors are applied to individual GNFR sectors.
This results in $24 * 7 * 12 = 2016$ samples per city per year.
The resulting dataset size is thus $2xx * 2 * 2016 = $ samples.
To reduce the memory overhead, scaling factors are applied at sampling time and only the original data is kept in memory.

\subsection{Dataset split}
The emission fields are split into training, validation, and test data.
The training data is used to train the model and updates its weights and makes up $\sim  70\%$ of the data.
The validation data is used to evaluate the generalization capabilities of the model during training and makes up $\sim  15\%$.
The test data is used for subsequent experiments and evaluations and makes up $\sim  15\%$.

For splitting, the emission fields are first sorted alphabetically by the city name.
Then, the first $15\%$ of cities are assigned to the test data.
This makes the split deterministic and thus reproducible.

The remaining cities are then randomly shuffled.
Finally, the first $\frac{0.15}{0.85}$ of cities are assigned to the validation set.
The remaining cities are assigned to the training set.

Maybe make a diagram here.

The number of samples in training, validation, and test set is given in table (make table).
A list of the cities in the test set is appended in appendix (add appendix).

\subsection{Data Augmentation}
To improve the generalization abilities of the model CITE, common image augmentation techniques are applied (to cite) to the emission fields in the training data.
First, random cropping is used.
Furthermore, a random horizontal flip and vertical flip are applied with probability $0.5$ each.
Lastly, emission fields are randomly rotated by 90$\circ$ with probability of $0.5$.
This results in $8$ different transformations, with one of them being the original emission field.

The validation and test emission fields are not transformed, except for a center crop.

\subsection{Scaling}
Scaling is an important factor for machine learning.
Large-scale data can make training converge slow and instable (cite).
Furthermore, regularization techniques such as batch normalization rely on scaled values (cite).
A common technique for scaling values is min max scaling (cite).
However, min max scaling is sensitive to outliers and thus not ideal for emission inventories where range of emissions within a city can largely differ.
Instead, robust scaling (cite) can be applied.
To determine a good scaling factor, the $95$th percentile of values is determined for each city in the training set.
The inverse of the average is then used as the scaling factor.
The resulting scaling factor is thus $\frac{1 a}{2.5 * 10^6 kg}$

\section{Variational Autoencoder}

\subsection{Architecture}

\section{Evaluation}

\subsection{Gaussian Measurements}
In order to evaluate the generative capbailities in the context of inverse problems, a compressed sensing problem is used for evaluation.
This compressed sensing problem is based on the paper by Bora et al. as they mention that the performance of generative models can be assesed based on their provided problem statement.
For this, a identically independent distributed matrix $A \in R^{m \times 15000}$ is sampled for each run.
The following inverse problem is then solved using the minimization problem proposed by Bora et al.
\begin{equation}
    y = A x + \epsilon
\end{equation}
with $x$ being the emission fields $x^*$ in vectorized form.
This inverse problem is equivalnt to taking taking $m$ measurements that is randomly linearly affected by any sector in any cell within the emission field grid.
While this does not correspond to a typical transport model which follows the law of physics, this problem serves as a good proxy for evaluating the generative capbailities in the context of inverse problems of the trained VAE. 

The evaluation is performed on scaled emission fields.

The resulting pipeline is the following:
\begin{enumerate}
    \item Generate random A
    \item Sample x from test set
    \item Vectorize x
    \item Compute y from forward model
    \item Run reconstruction algorithm (minimization problem)
    \item Unvectorize resulting x dach
    \item Compare x with x dach
\end{enumerate}

Let $D$ be the decoder of the variational autoencoder.
Then, the generator $G: R^k \rightarrow R^{15000}$ can be written as $G(z) = \text{vec}(D(z))$, i.e. the generator G is the vectorization of the decoder of the VAE.

The minimization problem
\begin{equation}
    z^* = \arg\min_{z}{\norm{A G(z) - y} + R(z)}
\end{equation}
with $R(z) = \norm{z}$ is solved using the Adam Optimizer (cite).
For the learning rate, values are chosen based on the number of measurements, as seen in table (make table).
These values are determined empirically.

The reconstruction algorithm is run for the following number of measurements:
For each of the numbers, the reconstruction is run for each emission field in the test dataset.
A random measurement matrix $A$ is generated and each of the $3$ algorithms solve the same inverse problem.
The temporal transforms are disabled during evaluation which means that for each city only one emission field per year is used.
The evaluation is run 5 times to reduce randomness resulting from random initialization of $z$.

\subsection{Transport Model Measurements}
