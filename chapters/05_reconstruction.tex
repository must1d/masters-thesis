% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Compressed Sensing}\label{chapter:compressed_sensing}

\section{Inverse Problems}
To evaluate the generative capbailities in the context of inverse problems, compressed sensing problems are used for evaluation.
The compressed sensing problem is defined using a sensing matrix $A \in R^{m \times 15360}$.
The forward model is then defined as
\begin{equation}
    y = A x + \epsilon
\end{equation}
The signal $x \in R^{15360}$ is derived from an emission inventory $\hat{x} \in R^{32 \times 32 \times 15}$ that is vectorized, i.e. $x = \text{vec}(\hat{x})$.
The Gaussian noise $\epsilon \in R^m$ is sampled from a zero mean Gaussian distribution $N(0, \sigma)$.
The standard deviation of $\sigma \in R$ depends on a paramtrizable signal to noise ratio SNR and the signal power $SP = \frac{1}{m}\sum_{i=1}^m{\left(Ax\right)_i}$ of the measurements.
\begin{equation}
    \sigma = \sqrt{\frac{SP}{SNR}}
\end{equation}

\subsection{Gaussian Measurements}
This compressed sensing problem is based on the paper by Bora et al. as they mention that the performance of generative models can be assesed based on their provided problem statement.
For this, a identically independent distributed matrix $A \in R^{m \times 15000}$ is sampled for each run.
The following inverse problem is then solved using the minimization problem proposed by Bora et al \parencite{CSUsingAI}.

with $x$ being the emission fields $x^*$ in vectorized form.
This inverse problem is equivalnt to taking taking $m$ measurements that is randomly linearly affected by any sector in any cell within the emission field grid.
While this does not correspond to a physical transport model which follows the law of physics, this problem serves as a good proxy for evaluating the generative capbailities in the context of inverse problems of the trained VAE. 
The randomly sampled sensing matrix fulfills the ... properties and thus allows comparision of solvers in ideal scenarios.
This allows assessing the the potential of solvers independent of outside variables.

The evaluation is performed on scaled emission fields.

The resulting pipeline is the following:
\begin{enumerate}
    \item Generate random A
    \item Sample x from test set
    \item Vectorize x
    \item Compute y from forward model
    \item Run reconstruction algorithm (minimization problem)
    \item Unvectorize resulting x dach
    \item Compare x with x dach
\end{enumerate}

The reconstruction algorithm is run for the following number of measurements:
For each of the numbers, the reconstruction is run for each emission field in the test dataset.
A random measurement matrix $A$ is generated and each of the $3$ algorithms solve the same inverse problem.
The temporal transforms are disabled during evaluation which means that for each city only one emission field per year is used.
The evaluation is run 5 times to reduce randomness resulting from random initialization of $z$.

\subsection{Gaussian Plume Model}
Gaussian sensing matrix do not represent the real the emission problem well.
Instead of randomly samples sensing matrices, transport models should be used that indicate the sensitivity of measurements with resepect to physical grid cells based on the transport of molecules in the past.
These transport models are computationally expensive and in generel difficult to estimate on a per city basis.
Thus, we apply the same idea as Benji in his work.
We substitue typical transport models, such as STILT, with a Gaussian plume model.
We want to apply the model to reconstruct total emissions from area sources, i.e. 32 by 32.
The generative models are trained to reconstruct sector-wise.
Thus, we can reconstruct the decomposition into sectors and the sum each sector to achieve reconstruction of total emissions.
This is a more challenging task for the generative model.
If it were directly trained on generating 32 by 32 fields, the results would be much better.

Furthermore, we consider the case of make measurements from the field containing point sources.
Measurements are thus computed as follows:
\begin{equation}
    y = A x + \epsilon
\end{equation}
The emissions can now be decomposed into area and point source fields.
\begin{align}
    y &= A (x_A + x_P) + \epsilon \\
    y &= A x_A + A x_P + \epsilon \\
    y - A x_P &= A x_A + \epsilon \\
    \tilde{y} &= A x_A + \epsilon
\end{align}

\subsection{Gaussian Plume Model 2}

In his paper, Benji reconstructed the emission field without considering the indivudal sectors as contributors.
In this thesis, a full reconstruction is attempted, i.e. all $15$ sectors are reconstructed.
Therefore, the sensing matrix based on the Gaussian plume model must be adapted accordingly.

Consider measurement in the spatial domain, i.e. in the $32$ by $32$ grid.
A measurement would then be affected by the sum of all emitters of each sector.
We denote $x_S = x_A + x_B + \dots x_L$, where $x_i$ corresponds to the emissions of sector $i \in \{A, B, \dots, L\}$, as the sum of all sectors.
Then, the forward sensing model has the following form.
\begin{equation}
    y = A_S x_S + \epsilon
\end{equation}
This can be formulated as follows:
\begin{align}
    y &= A_S (x_A + x_B + \dots + x_L) + \epsilon \\
    y &= A_S x_A + A_S x_B + \dots + A_S x_L + \epsilon \\
\end{align}
Rewriting this as a matrix multiplication yields the following
\begin{equation}
    y = \begin{bmatrix} A_S & A_S & \cdots & A_S \end{bmatrix}\begin{bmatrix} x_A \\ x_B \\ \vdots \\x_L \end{bmatrix} + \epsilon
\end{equation}
This can now be reforumalated to the following sensing problem:
\begin{equation}
    y = Ax + \epsilon
\end{equation}
with $A = \begin{bmatrix} A_S & A_S & \cdots & A_S \end{bmatrix}$.
Stacking each sector $x_i$ yields the vectorized emission inventory.
It can thus be concluded that for sector-wise reconstruction, the sensing matrix $A = \begin{bmatrix} A_S & A_S & \cdots & A_S \end{bmatrix}$ can be used.

\section{Inverse Problem Solvers}

\subsection{Lasso}
To assess the capabilities of the generative model against other approaches, the sparse reconstruction algorithm using the Lasso regularization \parencite{Lasso} is used.
\begin{equation}
    Lasso = 
\end{equation}
Scaling factor $\alpha$ is chosen as $0.1$, like in Bora et als paper.
In addition to the Lasso in the original basis, the problems is transformed into two further bases.
The first transform used for this is the discrete cosine transform.
The cosine coefficients are computed
The second transform is the discrete wavelet transform.
Wavelet coefficients are computed using \parencite{PyWavelets}.
Transformations are applied individually per sector.

\subsection{Generative Model Solver}
Let $D: R^d \rightarrow R^{32 \times 32 \times 15}$ be the decoder of the variational autoencoder.
Then, the generator $G: R^d \rightarrow R^{15360}$ can be written as $G(z) = \text{vec}(D(z))$, i.e. the generator G is the vectorization of the decoder of the VAE.

The minimization problem
\begin{equation}
    z^* = \arg\min_{z}{\left( \norm{A G(z) - y}_2^2 + \alpha R(z) \right)}
\end{equation}
with regularization term $R(z) = \norm{z}_2^2$ is solved numerically using the Adam Optimizer \parencite{Adam}.
% For the learning rate $\alpha$, values are chosen based on the number of measurements $m$ as the sensing matrix increases in dimensions.
% With an increase in dimensions of the sensing matrix, the gradients also scale.
% The learning rates can be seen in Table \ref{table:1}.
% \begin{table}[h!]
%     \centering
%     \begin{tabular}{|c|c c c c c c c c c|}
%         \hline
%         $m$ & $50$ & $100$ & $250$ & $500$ & $1000$ & $2500$ & $5000$ & $10000$ & $12500$ \\
%         \hline
%         $\alpha$ & $1.5 * 10^{-4}$ & $5 * 10^{-4}$ & $10^{-3}$ & $1.8 * 10^{-3}$ & $2.5 * 10^{-3}$ & $4 * 10^{-3}$ & $5.5 * 10^{-3}$ & $7 * 10^{-3}$ & $1.5 * 10^{-2}$ \\
%         \hline
%     \end{tabular}
%     \caption{Learning Rate $\alpha$ Based on Number of Measurements $m$}
%     \label{table:1}
% \end{table}
% These values are determined empirically.
For the problems in this thesis, $\alpha = 0.1$ showed the best results and is thus used for subsequent experiments.
With $z^*$ determined, the generative model solver returns as $D(z^*)$ as its reconstruction.

\subsection{Sparse Generative Model Solver}
The hyperparamater lambda is optimized by investigating the loss curve.
It should be smooth and taper off slowly.
